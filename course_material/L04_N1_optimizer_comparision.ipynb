{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c82e207-9875-4617-8046-8aa0f4f01d45",
   "metadata": {},
   "source": [
    "# Optimizer Comparison Exercise\n",
    "## Objective\n",
    "Explore how different optimization algorithms affect the training of neural networks.\n",
    "\n",
    "## Background\n",
    "Optimizers are algorithms or methods used to change the attributes of the neural network, such as weights and learning rate, to reduce the losses. Common optimizers include SGD (Stochastic Gradient Descent), Mini-batch SGD, Momentum, Adam, and RMSProp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b17beb-e01c-417a-a14e-56e70adf0bd6",
   "metadata": {},
   "source": [
    "# Part 1: Classification task\n",
    "\n",
    "## Setup\n",
    "Start by importing necessary libraries and preparing a simple dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cb463e0-5770-4e6d-a618-3b815386c1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\GitHub\\deep_learning\\venv\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load or create a dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954232b6-ab88-4616-94c8-808c931b9224",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "Define a function to create a basic neural network. This function will take an optimizer as an argument. Define the network yourself with 2 hidden layers using 64 nodes each. They should both use relu as activation function. Dont forget the output layer which, for a binary classification problem should use what activation function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21b4afa7-446c-445f-abc9-597ecc04920f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2194585712.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    model =\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def build_model(optimizer):\n",
    "    # Add your model here following the above instructions.\n",
    "    model = tf.keras.Sequential()\n",
    "\n",
    "    model.add(layer())\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af8c115-2569-4e3b-a986-dca054cfdcfb",
   "metadata": {},
   "source": [
    "## Experimentation\n",
    "Experiment with different optimizers. For each optimizer, train the model and plot its accuracy and loss.\n",
    "\n",
    "### Task:\n",
    "1. Try the following optimizers: 'sgd', 'adam', 'rmsprop', and a custom SGD with Momentum.\n",
    "2. For the custom SGD with Momentum, use: tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "3. Train each model for a fixed number of epochs (e.g., 30) and evaluate its performance on the test set.\n",
    "4. Do step 1-3 same, but change batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125645a2-f848-44a5-af82-0d1b8032c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = ['sgd', 'adam', 'rmsprop', 'SGD_momentum']\n",
    "histories = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training with optimizer: {opt}\")\n",
    "    if opt==\"SGD_momentum\": \n",
    "        model = build_model(tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9))\n",
    "    else:\n",
    "        model = build_model(opt)\n",
    "    history = model.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), verbose=0)\n",
    "    histories[str(opt)] = history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6efce69-ebb2-44ed-9eb3-a051718eb7db",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Plot the training and validation accuracy and loss for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b84723-8673-4411-ae88-024e81fea744",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in histories.items():\n",
    "    plt.plot(history.history['accuracy'], label=f'{opt} - Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label=f'{opt} - Validation Accuracy')\n",
    "plt.title('Model Accuracy by Optimizer')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in histories.items():\n",
    "    plt.plot(history.history['loss'], label=f'{opt} - Loss')\n",
    "    plt.plot(history.history['val_loss'], label=f'{opt} - Validation Loss')\n",
    "plt.title('Model Loss by Optimizer')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccce81-02f4-454d-92b1-407b051730c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Analysis and Questions\n",
    "* Which optimizer provided the fastest convergence?\n",
    "* Which optimizer achieved the highest accuracy on the validation set?\n",
    "* Discuss the possible reasons behind the performance differences observed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e970bb7-e441-46f2-9764-b57f8a5c2715",
   "metadata": {},
   "source": [
    "# Part 2: Regression task\n",
    "Now, let's apply the same set of optimizers to a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903a493-2f05-444f-ab1c-e8d0fac4d1e4",
   "metadata": {},
   "source": [
    "## Setup for Regression\n",
    "Import libraries and prepare a regression dataset. For simplicity, let's use a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ccc21-6c09-43fb-a8fa-651587416c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data for regression\n",
    "X_reg, y_reg = make_regression(n_samples=500, n_features=2, noise=15, random_state=42)\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff2ade-85b1-4378-9c7b-d7361d32c428",
   "metadata": {},
   "source": [
    "## Define the Neural Network Model for Regression\n",
    "Create a function to build a neural network model suitable for regression. Use 2 hidden layeres of 10 nodes each both using relu activations. Dont forget the output layer. For regression it is recommended to use MSE as loss and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eec899-b33e-483f-adf1-31a98733d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_regression(optimizer):\n",
    "    # -- Define the model yourself here following the instuctions above.\n",
    "    model = \n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mse'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dd0b4f-7ea3-4786-80b1-bafb694af0ce",
   "metadata": {},
   "source": [
    "## Experiment with Different Optimizers for Regression\n",
    "Repeat the same process as in the classification task, but now for regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d043ae90-8b6f-4455-b2fd-c5e07c9daa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict_reg = {}\n",
    "\n",
    "for opt in optimizers:\n",
    "    print(f\"Training (Regression) with optimizer: {opt}\")\n",
    "    if opt==\"SGD_momentum\": \n",
    "        model = build_model_regression(tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9))\n",
    "    else:\n",
    "        model = build_model_regression(opt)\n",
    "    history = model.fit(X_train_reg, y_train_reg, epochs=100, validation_split=0.2, verbose=0)\n",
    "    history_dict_reg[opt] = history\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss, test_mse = model.evaluate(X_test_reg, y_test_reg, verbose=0)\n",
    "    print(f\"Test MSE with {opt}: {test_mse:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928c8024-0fbb-47c6-959d-b1d703c50231",
   "metadata": {},
   "source": [
    "## Visualization of Results for Regression\n",
    "Plot the training and validation loss (MSE) for each optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b852e6-983b-4b29-896b-a8e5fce2c06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "for opt, history in history_dict_reg.items():\n",
    "    plt.plot(history.history['mse'], label=f'{opt} - Train (Reg)')\n",
    "    plt.plot(history.history['val_mse'], label=f'{opt} - Val (Reg)')\n",
    "\n",
    "plt.title('Model MSE with Different Optimizers (Regression)')\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ca69e-96ae-47fa-a067-7c99dac457ad",
   "metadata": {},
   "source": [
    "## Analysis and Questions\n",
    "* Compare the performance of the optimizers between the classification and regression tasks.\n",
    "* Did certain optimizers perform better on one task than the other? Why might this be?\n",
    "* Discuss the implications of these findings for selecting optimizers in real-world applications.\n",
    "\n",
    "## Conclusion\n",
    "Reflect on the importance of understanding the strengths and limitations of different optimizers in relation to the specific nature of the problem and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0288ad-a1cf-4be6-9b68-a74ddcdf821b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39e657b-072f-4f17-9d0d-224b55867a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104dc1a7-6cbe-4b76-82a2-74d647ea6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
